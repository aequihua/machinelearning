---
title: "Practical Machine Learning - Project Course"
author: "Arturo Equihua"
date: "June 11, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Synopsis
This document contains the details of the definition of a random-tree based machine learning model, which was used to predict the expected quality of workout ("how well weightlifting is done") based on a number of arm movement measures that can be read with the help of specialized devices (e.g. Nike Fuelband). 

The resulting model takes over 50 predictor variables, and the outcome is a predicted "classe" that can take the values of A to E, where A is the "perfect" exercise and the other values represent common mistakes made by people when doing weight training. The accuracy of the model is higher than 99%, which is deemed good.

##Introduction
This document describes a proposed model that was built to predict the "quality of workout done" (well done or done with mistakes), based on several variables that were measured on subjects wearing special devices (Nike Fuelband and so on), while performing specific weightlifting movements.

The source data comes from the following study, which also has additional information about the nature of it: 

*Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human) . Stuttgart, Germany: ACM SIGCHI, 2013.*

##Dataset Structure and Preparation
The input data comes in CSV format, in two files (training set and testing set). The variable that we will predict is the "classe" variable (Class A means "well done", and classes B, C, D and E represent specific types of mistakes).

> More information about the dataset is available in <http://groupware.les.inf.puc-rio.br/har>, in the section "Weight Lifting Exercise Dataset".

The code to load the training and testing data is shown below:
```{r load_data, echo=TRUE, results='hide', warning=FALSE, cache=FALSE}
  library(caret)
  library(rpart)
  library(e1071)
  library(rpart.plot)
  wltrain <- read.csv("pml-training.csv")
  wltest <- read.csv("pml-testing.csv")
```
In order to select the features for the prediction model  - the entire set has 160 variables that could be used - , a quick look at the training dataset suggests there are variables that could be removed as they have many NA values in them. We can also eliminate variables that are not relevant for the problem we are trying to solve, or have little variance (repeat the same value too much). We then do the variable cleansing as shown below:

```{r clean_data, echo=TRUE, results='hide', warning=FALSE, cache=FALSE}
  # remove variables that have many NA
  manyNA <- sapply(wltrain, function(x) mean(is.na(x))) > 0.95
  wltrain <- wltrain[, manyNA==F]
  wltest <- wltest[, manyNA==F]

  # remove variables with nearly zero variance
  nzv <- nearZeroVar(wltrain)
  wltrain <- wltrain[, -nzv]
  wltest <- wltest[, -nzv]

  # remove variables that don't make intuitive sense for prediction   (X, user_name, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp), which happen to be the first five variables
  wltrain <- wltrain[, -(1:5)]
  wltest <- wltest[, -(1:5)]
```

Once the unneeded variables are removed, the training set is split into two subsets so that we can estimate the out-of-sample error properly, without having to use the testing dataset. 
```{r split_data, echo=TRUE, results='hide', warning=FALSE, message=FALSE,cache=FALSE}
  set.seed(10)
  inTrain <- createDataPartition(y=wltrain$classe, p=0.7, list=FALSE)
  wltrain1 <- wltrain[inTrain, ]
  wltrain2 <- wltrain[-inTrain, ]
```

##Model Building
Given the nature of the arm movement when doing weight lifting, intuitively it makes sense to assume that the outcome (classe) is a function of several variables at once. Our dataset has a large number of variables that could all be used, so the initial choice for modelling method is *Random Forest*. This approach can take several variables into account, doing bootstrapping (resampling), does not assume linearity and it averages different decision trees to achieve more accuracy. 

The following code performs the training process using the random forest method, on the subset of the training dataset obtained beforehand:

```{r model_data, echo=TRUE, results='hide', warning=FALSE, message=FALSE,cache=FALSE}
  # instruct train to use 3-fold CV to select optimal tuning parameters
  fitControl <- trainControl(method="cv", number=3, verboseIter=F)
  # fit model on wltrain1
  fit <- train(classe ~ ., data=wltrain1, method="rf", trControl=fitControl)
```

##Model Final Version
In order to decide upon the quality of the model, we now apply it to predict the "test data" (the subset of the training data set that was reserved). This is the prediction and the confusion matrix to assess the accuracy:

```{r predict_first, echo=TRUE, warning=FALSE, message=FALSE,cache=FALSE}
  preds <- predict(fit, newdata=wltrain2)
  cf <- confusionMatrix(wltrain2$classe, preds)
  cf
```

It can be seen that the accuracy of this model is `r cf$overall[1]` , which is considered adequate. So it is decided to keep this modelling method to predict the values of the test data.

To that end, now it is time to retrain the model with the same random forest method, using the entire training dataset. In the following code we create this adjusted model:

```{r retrain_model, echo=TRUE, warning=FALSE, message=FALSE,cache=FALSE}
    fitControl <- trainControl(method="cv", number=3, verboseIter=F)
    fit <- train(classe ~ ., data=wltrain, method="rf", trControl=fitControl)
```

The below shows the fitted model result:

```{r display_model, echo=FALSE, warning=FALSE, message=FALSE,cache=FALSE}
   fit$finalModel
```

##Final Prediction on Test Dataset
The following is the code to run the predictions over the test dataset, and to display the results:

```{r predict_test, echo=TRUE, warning=FALSE, message=FALSE,cache=FALSE}
  # predict on test set
  preds <- predict(fit, newdata=wltest)
  print(preds)
```

According to the results of the confusion matrix above, the accuracy of the model looks excellent and can be applied to further datasets.

Finally, the predictions on the test dataset are executed and saved into text files so that they are fed to the class evaluation:

```{r predict_savetofiles, echo=TRUE, warning=FALSE, message=FALSE,cache=FALSE}
  preds <- as.character(preds)
  
  # create function to write predictions to files
  pml_write_files <- function(x) {
      n <- length(x)
      for(i in 1:n) {
          filename <- paste0("problem_id_", i, ".txt")
          write.table(x[i], file=filename, quote=F, row.names=F, col.names=F)
      }
  }
  
  # create prediction files to submit
  pml_write_files(preds)
```

